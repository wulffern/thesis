%DRAFT VERSION
%\documentclass[peerreviewca,12pt,a4paper,draft]{IEEEtran}
%\newcommand{\myfigwidth}{\textwidth}
%\def\baselinestretch{2}

\chapter{Introduction to the mathematics of noise sources}
\label{ap:noise}

This is a compilation of different books \cite{ziel}, \cite{johns},
\cite{razavi} and
their introduction to noise analysis of electronic circuits.


\section{Noise}
Noise is a phenomena that occurs in all electronic circuits. It
places a lower limit on the smallest signal we can use. Many now have super audio compact disc (SACD)
players with 24bit
converters, 24 bits is around $2^{24} = 16.78$ Million different
levels. If 5V is the maximum voltage, the minimum would have to be
$\frac{5V}{2^{24}} \approx 298nV$. That level is roughly equivalent to the noise in a 50
Ohm resistor with a bandwidth of 96kHz. There exist an equation that relates number of bits to
signal to noise ratio \cite{johns}, the equation specifies that $SNR =
6.02 \times Bits +
1.76 = 146.24dB$. As of 12.2005 the best digital to analog converter (DAC)
that Analog Devices  has is a DAC
with 120dB SNR, that equals around $Bits = (120-1.76)/6.02 =
19.64$. In other words, the last four bits of your SACD player is probably
noise!

%-------------------------------------------------------
\section{Statistics}
%-------------------------------------------------------

The mean of a signal x(t) is defined as
\eqn{
\label{mean}
  \overline{x(t)} = \powavg{x(t)}
}
The mean square of x(t) defined as 
\eqn{
\label{eq:meansquare}
  \overline{x^2(t)} =\powavg{x^2(t)}
}
The variance of x(t) defined as
\eqn{
\label{eq:var}
  \sigma^2 = \overline{x^2(t)} - \overline{x(t)}^2
}
For a signals with a mean of zero the variance is equal to the mean
square.
The auto-correlation of x(t) is defined as
\eqna{
\label{eq:autocor}
  R_x(\tau ) &{}={}& \overline{x(t)x(t + \tau)} \nonumber\\
&{}={}& \: \powavg{x(t)x(t+\tau)}
%   & =& \lim_{T\to\infty}\frac{1}{T} \int_{-\infty}^{\infty}{x(t)x(t- \tau) dt} \\
}

%-------------------------------------------------------
\section{Average power}
%-------------------------------------------------------
Average power is defined for a continuous system as \req{powcont} and
for discrete samples it can be defined as \req{powsamp}. $P_{av}$
usually has the unit $A^2$ or $V^2$, so we have to multiply/divide by the
impedance to get the power in Watts. To get Volts and Amperes we use
the root-mean-square (RMS) value which is defined as $\sqrt{P_{av}}$. 

\eqn{\label{eq:powcont}
P_{av} = \lim_{T\to\infty} \frac{1}{T} \int^{+T/2}_{-T/2} x^2(t) dt
}

\eqn{\label{eq:powsamp}
P_{av} = \frac{1}{N}\sum_{i=0}^N x^2(i)
}

If x(t) has a mean of zero then, according to \req{var}, $P_{av}$ is
equal to the variance of x(t).

Many different notations are used to denote average power and RMS value of
voltage or current, some of them are listed in Table \ref{t:avgpow}
and Table \ref{t:rms}. Notation can be a confusing thing, it changes
from book to book and makes expressions look different. It is
important to realize that it does not matter how you write average
power and RMS value. If you want, you can invent your own notation for
average power and RMS value. If you, however, are presenting your
calculations to other people it is convenient if they understand what
you mean. In the remainder of this section we will use $
\overline{e_n^2}$ for average power when we talk about voltage noise source  and
$\overline{i_n^2}$ for average power when we talk about current noise source. The
$n$ subscript is used to identify different sources.

\begin{table}[ht]
\centering 
\begin{minipage}[b]{0.48\columnwidth}%
\centering 
\caption{Notations for average power}
\begin{tabular}{cc}
\label{t:avgpow}
Voltage&Current\tabsp
$V_{rms}^2$&$I_{rms}^2$\tabsp
$\overline{V_n^2}$&$\overline{I_n^2}$\tabsp
$\overline{v_n^2}$&$\overline{i_n^2}$\tabsp
\end{tabular}
\end{minipage}%
\begin{minipage}[b]{0.48\columnwidth}%
\centering 
\caption{Notations for RMS}
\begin{tabular}{cc}
\label{t:rms}
Voltage&Current\tabsp
$V_{rms}$&$I_{rms}$\tabsp
$\sqrt{\overline{V_n^2}}$&$\sqrt{\overline{I_n^2}}$\tabsp
$\sqrt{\overline{v_n^2}}$&$\sqrt{\overline{i_n^2}}$\tabsp
\end{tabular}
\end{minipage}
\end{table}

%-------------------------------------------------------
\section{Noise spectrum}
%-------------------------------------------------------
With random noise it is useful to relate the average power to
frequency. We call this Power Spectral Density (PSD). The PSD show how
much power a signal carries at each frequency. In the literature $S_x(f)$
is often used to denote the PSD. In the same way
that we use $V^2$ as unit of average power, the unit of the PSD is
$\frac{V^2}{Hz}$ for voltage and $\frac{A^2}{Hz}$ current. The root
spectral density is defined as $\sqrt{S_x(f)}$ and has unit
$\frac{V}{\sqrt{Hz}}$ for voltage and $\frac{I}{\sqrt{Hz}}$ for
current. 

%A circuit which you will always find in a radio
%frequency receiver is a low noise amplifier (LNA). When we specify how much
%noise the LNA should contribute we can say e.g
%$5n\frac{V}{\sqrt{Hz}}$. This specification does not say what type of noise or where
%the noise comes from, only how much RMS voltage it should contribute
%per square root of hertz. 

The power spectral density is defined as two times the Fourier transform of the
auto-correlation function \cite{ziel}
\eqn{
\label{eq:psd}
S_x(f) = 2\inti{R_x(\tau)e^{-j2\pi f \tau}d\tau}
}
This can also be written as 
\eqna{
S_x(f) &{}={}& 2\left[\inti{R_x(\tau)\cos(\omega \tau)d\tau} - \inti{R_x(\tau)j\sin(\omega  \tau)d\tau}\right] \nonumber\\
&{}={}& 2\left[\intio{R_x(\tau)\cos(\omega \tau)d\tau} +\intoi{R_x(\tau)\cos(\omega \tau)d\tau}\right] \nonumber \\
&{}-{}& 2j\left[\intio{R_x(\tau)\sin(\omega \tau)d\tau} +  \intoi{R_x(\tau)\sin(\omega \tau)d\tau} \right]  \nonumber \\
&{}={}& 4\intoi{R_x(\tau)\cos(\omega \tau)d\tau} \nonumber \\
&{}-{}& 2j\left[- \intoi{R_x(\tau)\sin(\omega \tau)d\tau} +  \intoi{R_x(\tau)\sin(\omega \tau)d\tau} \right] \nonumber \\
&{}={}& 4\intoi{R_x(\tau)\cos(\omega \tau)d\tau}
}
since $e^{-j\omega \tau} = \cos(\omega \tau) - j \sin (\omega
  \tau)$,  $R_x(\tau)$ and $\cos(\omega \tau)$ are symmetric around
  $\tau=0$ while $\sin(\omega \tau)$ is asymmetric around $\tau = 0$.

The inverse of power spectral density is defined as
\eqn{
\label{eq:autopsd}
R_x(\tau)  = \frac{1}{2}\inti{S_x(f)e^{j 2 \pi f \tau} df} = \intoi{S_x(f) \cos(\omega \tau)df}
}

If we set $\tau=0$ we get
\eqn{
\label{eq:ms_psd}
  \overline{x^2(t)} = \intoi{S_x(f)df}
}
which means we can easily calculate the average power if we know the
power spectral density.
As we will see later it is common to express noise sources in PSD
form. 

Another very useful theorem when working with noise in the frequency
domain is
\eqn{
\label{eq:psd_hf}
  S_y(f) = S_x(f)|H(f)|^2
}
where $S_y(f)$ is the output power spectral density, $S_x(f)$ is the
input power spectral density and $H(f)$ is the transfer function of
a time-invariant linear system. 

If we insert \req{psd_hf} into  \req{ms_psd}, with $S_x(f) =
a\:constant = D_v$
we get
\eqn{
  \overline{x^2(t)} = \int{S_y(f)df} = D_v\int{|H(f)|^2 df} = D_v f_x
}
where $f_x$ is what we call the noise bandwidth. For a single
time constant RC network the noise bandwidth is equal to
\eqn{
  f_x = \frac{\pi f_0}{2} = \frac{1}{4 R C}
}
where $f_x$ is the noise bandwidth and $f_0$ is the 3dB frequency.

Thermal noise is white and white
means that the power spectral density is flat (constant over all
frequencies).  If $S_x(f)$ is our thermal noise source
and $H(f)$ is a standard low pass filter, then equation \req{psd_hf} tells
us that the output spectral density will be shaped by $H(f)$. At
frequencies above the $f_x$ in $H(f)$ we expect the root power spectral
density to fall by 20dB per decade. 


%-------------------------------------------------------
\section{Probability distribution}
%-------------------------------------------------------

\begin{theorem}[Central limit theorem]
The sum of $n$ independent random variables subjected to the same
distribution will always approach a normal distribution curve as $n$ increases.
\end{theorem}

This is a neat theorem, it explains why many noise sources we
encounter in the real world are white.\footnote{Gaussian distribution
  = normal distribution. Noise sources with Gaussian distribution are
  called white} Take thermal noise for example,
it is generated by random motion of carriers in materials. If we look
at a single electron moving through the material the probability
distribution might not be Gaussian. But summing probability
distribution of the random movements with
a large number of electrons will give us a Gaussian distribution, thus
thermal noise is white.

%-------------------------------------------------------
\section{PSD of a white noise source}
%-------------------------------------------------------
If we have a true random process with Gaussian distribution we know that
the auto-correlation function only has a value for $\tau=0$. From
equation \req{autocor} we have that
\eqna{
R_x(\tau ) &{}={}&{}  \powavg{x(t)x(t - \tau)} \nonumber\\
&{}={}&{} \left[ \powavg{x^2(t)} \right] \delta(\tau) \nonumber\\
&{}={}&{}\: \overline{x^2(t)}\delta(\tau)
}
The reason being that in a true random process $x(t)$ is uncorrelated
with $x(t + \tau )$. If we use equation \req{psd} we
see that 
\eqna{
  S_x(f) &{}={}&\: 2\inti{\overline{x^2(t)}\delta(\tau)e^{-j 2 \pi f \tau} d\tau} \nonumber\\
&{}={}&\:2\overline{x^2(t)} \inti{\delta(\tau)e^{-j 2 \pi f \tau}
    d\tau} \nonumber \\
&{}={}& 2\overline{x^2(t)}
}
since
\eqn{
\int{\delta(\tau)e^{-j 2 \pi f \tau} d\tau} = e^0 = 1
}
 This means that the power spectral
density of a white noise source is flat, or in other words, the same
for all frequencies.

%-------------------------------------------------------
\section{Summing noise sources}
%-------------------------------------------------------

Summing noise sources is usually trivial, but we need to know why and when it is not.
We write the  time dependant noise signals as 
\eqn{
  v_{tot}^2(t) = (v_1(t) + v_2(t))^2 = v_1^2(t) + 2v_1(t)v_2(t) + v_2^2(t)
}
The average power of this is defined as
\eqna{
\label{eq:noisesum}
  \en{tot} &{}={}& \powavg{v_{tot}^2(t)}  \nonumber\\
&{}={}& \powavg{v_1^2(t)} \nonumber\\
&{}+{}&  \powavg{v_2^2(t)} \nonumber\\
&{}+{}& \powavg{2v_1(t)v_2(t)} \nonumber\\
&{}={}& \en{1} + \en{2}
+ \powavg{2v_1(t)v_2(t)}
}

If $\en{1}$ and $\en{2}$ are uncorrelated noise sources we can skip
the last term in \req{noisesum} and just write
\eqn{
  \en{tot} = \en{1} + \en{2}
}
Most natural noise sources are uncorrelated.

%-------------------------------------------------------
\section{Signal to noise ratios}
%-------------------------------------------------------
Signal to Noise Ratio (SNR) is a common method to specify the relation
between signal power and noise power in linear systems. It is defined as
\eqna{
  SNR &{}={}& 10 \log\left(\frac{Signal\:power}{Noise\:power}\right)\nonumber\\
    &{}={}& 10 \log\left(\frac{\overline{v_{sig}^2}}{\en{n}}\right)\nonumber\\
  &{}={}&  20 \log\left(\frac{v_{rms}}{\sqrt{\en{n}}}\right)
}

Another useful ratio is Signal to Noise and Distortion (SNDR).
Since most real systems exhibit non-linearities it is useful to include
distortion in the ratio.  One can calculate SNR and SNDR in many
ways. If we don't know the expression for $\en{n}$ we can do a FFT of
our output signal. From this FFT we sum spectral components except at the
signal frequency to get noise and distortion. SNR is normally
calculated as
\eqn{
  SNR = 10
  \log\left(\frac{Signal\:power}{Noise\:power\:-\:6
\:first\:harmonics}\right)
}
And SNDR is calculated as
\eqn{
  SNDR = 10\log\left(\frac{Signal\:power}{Noise\:  power}\right)
}

%-------------------------------------------------------
\section{Noise figure and Friis formula}
%-------------------------------------------------------
Noise factor is a measure on the noise performance of a system. It is
defined as 
\eqn{
  F =
  \frac{\overline{v_o^2}}{source\:contribution\:to\:\overline{v_o^2}} 
}
where $\overline{v_o^2}$ is the total output noise. 

%Say we want to
%measure the noise factor of a black box. We connect
%a noise source, a known $\en{n}$, at the input and measure
%$\overline{v_o^2}$ at the output. The noise factor would then be 

%\eqn{
%  F = \frac{\overline{v_o^2}}{\en{n}}
%}
%and 
The noise figure is defined as (noise factor in dB)
\eqn{
  NF = 10 \log(F)
}
The noise factor can also be defined as
\eqn{
  F = \frac{SNR_{input}}{SNR_{output}}
}

This brings us right into what is known as Friis formula. If we have
a multistage system, for example several amplifiers in cascade, the
total noise figure of the system is defined as 

\eqn{
  F = 1 + F_1 - 1 + \frac{F_2 -1}{G_{1}} +
  \frac{F_3-1}{G_{1}G_{2}} + ....
}
Here $F_i$ is the noise figures of the individual stages and $G_i$ is
the available gain of each stage. This can be rewritten as 

\eqn{
  F = F_1 + \sum_{i=1}^N{\frac{F_{i+1} - 1}{\prod_{k=1}^{i-1}{G_{i}}}}
}

Friis formula tells us that it is the noise in the first stage that
is the most important if $G_1$ is large. We could say that in a system
it is important to amplify the noise as early as possible!

%-------------------------------------------------------
%\section{Conclusion}
%-------------------------------------------------------
%We have looked at the properties of noise in time domain and frequency
%domain. The equations in this appendix are useful tools when dealing with
%noise sources. 

% \section{References}
% \begin{enumerate}
% \renewcommand\labelenumi{[\theenumi]}
% \item \label{ziel}
% A.~V.~D. Ziel, \emph{Noise in Solid State Devices and Circuits}.\hskip 1em plus
%   0.5em minus 0.4em\relax John Wiley \& Sons Inc, 1986.

% \item \label{johns}
% D.~Johns and K.~Martin, \emph{Analog Integrated Circuit Design}.\hskip 1em plus
%   0.5em minus 0.4em\relax John Wiley \& Sons, Inc., 1997.

% \item \label{razavi}
% B.~Razavi, \emph{Design of Analog CMOS Integrated Circuits}.\hskip 1em plus
%   0.5em minus 0.4em\relax McGraw-Hill, 2001.
% \end{enumerate}




%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../wulff/work/ntnu/phd/thesis/tb_noise"
%%% End: 
